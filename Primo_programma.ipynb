{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Calcolo del numero di frasi e di token nel corpus**. Le tre funzioni permettono di:\n",
    "- leggere il contenuto del corpus e inserirlo in una variabile (`contents`), su cui lavoro per non accedere direttamente al testo\n",
    "- fare il sentence splitting del testo, le cui frasi finiscono nella variabile `sentences`, con la funzione di nltk `nltk.tokenize.sent_tokenize`\n",
    "- a partire dalle frasi (estratte nella seconda funzione), questa funzione seleziona i token per ogni frase con una iterazione e li inserisce con l'operazione `.append` in un nuovo array (`tokens_in_sentences`) dove ogni frase è una lista di liste (ognuna al proprio interno ha i suoi token), mentre con l'operazione `.extend` inserisco tutti i token in una unica lista (`all_tokens`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def read_file_content(file):\n",
    "    with open(file, 'r', encoding=\"utf8\") as infile:\n",
    "        contents = infile.read()\n",
    "        return contents\n",
    "\n",
    "def get_sentences(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def get_tokens(text_sentences):\n",
    "    all_tokens = []\n",
    "    tokens_in_sentences = []\n",
    "    for sentence in text_sentences:\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        all_tokens.extend(tokens)\n",
    "        tokens_in_sentences.append(tokens)\n",
    "    return all_tokens, tokens_in_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Calcolo della Lunghezza media delle frasi in token e della lunghezza media dei token**. Le due funzioni permettono di:\n",
    "- calcolare, a partire dal numero di frasi nel corpus e dalle frasi tokenizzate, la lunghezza media in token della frase. Iterando sulle frasi tokenizzate, sotto forma di lista di liste, calcolo la lunghezza di ogni frase (=numero di token dentro ognuna) e sommo tutte le lunghezze inserendole in una variabile (`all_sentence_length`). Per fare la media divido questa variabile per tutte le frasi presenti nel corpus (`number_of_sentences`).\n",
    "- calcolare, a partire dai token del testo e dalla lunghezza del corpus, la lunghezza media dei token. Anche qui calcolo la lunghezza di ogni token sotto forma di lista di liste (in questo caso le lettere), la sommo alle altre inserendola in una variabile (`all_tokens_length`) e divido poi la variabile per il numero di token presenti nel corpus, che equivale alla lunghezza di quest'ultimo. Una cosa in più che è stata aggiunta è una condizione che mi permette, con una approsimazione, di escludere dall'analisi i segni di punteggiatura. In particolare la funzione aggiunge la lunghezza del token alla somma totale delle lunghezze solo se il token è più lungo di 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(number_of_sentences, tokens_in_sentences):\n",
    "    \n",
    "    all_sentences_length = 0\n",
    "\n",
    "    for sentence in tokens_in_sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        all_sentences_length = all_sentences_length + sentence_length\n",
    "    \n",
    "    return (all_sentences_length/number_of_sentences)\n",
    "\n",
    "def token_length(text_tokens, corpus_length):\n",
    "    \n",
    "    all_tokens_length = 0\n",
    "\n",
    "    for token in text_tokens:\n",
    "        if(len(token)!=1):\n",
    "            all_tokens_length = all_tokens_length + len(token)\n",
    "    return(all_tokens_length/corpus_length)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Calcolo del numero di hapax nei primi 500, 1000 e 3000 token e nell'intero corpus**. Ho utilizzato una unica funzione che permette di calcolare gli hapax in diversi punti del testo a partire dai token del corpus. All'inizio ho specificato i 4 punti del testo in cui voglio calcolare il numero degli hapax. Li metto in un array per poi creare un unico ciclo che, iterando su ognuno dei valori, mi calcola gli hapax del corpus che come lunghezza il valore che sta iterando. Per fare ciò ogni volta aggiorno il mio corpus (creando un `new_corpus`) attraverso l'operazione di slicing (`tokens[:x]`) che prende i valori di una lista fino all'indice `x`. Per trovare gli hapax in ognuno dei miei nuovi corpora utilizzo un ciclo che lavora sulla distribuzione di frequenze dei token nel testo a partire da una funzione predefinita di nltk (`nltk.FreqDist`). Questa funzione crea un dizionario che associa ad ogni token la sua frequenza. Itero su ogni token del dizionario e con una condizione specifico che mi servono i token con frequenza 1, inserendoli poi in un array (`hapax`). Infine aggiungo la lunghezza di ogni raccolta di hapax in un altro array (`hapax_length`), dove avrò i valori che cercavo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hapax(tokens):\n",
    "    inc = [500, 1000, 3000, (len(tokens)-1)]\n",
    "    hapax_length = []\n",
    "    for x in inc:\n",
    "        new_corpus = tokens[:x]\n",
    "        freq = nltk.FreqDist(new_corpus)\n",
    "        hapax = []\n",
    "        for token in freq:\n",
    "            if(freq[token] == 1):\n",
    "                hapax.append(token)\n",
    "        hapax_length.append(len(hapax))\n",
    "    return hapax_length\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.**Dimensione del vocabolario e ricchezza lessicale (TTR), calcolati per porzioni incrementali di 200 token**. La funzione `voc_TTR` prende in input i token di un corpus e con un ciclo `while` calcola vocabolario e TTR (vocabolario/lunghezza del corpus) ogni 200 token. L'indice `i` lo aggiorno quindi di 200 finché non arriva al valore della lunghezza di tutto il corpus e ogni volta cacolo vocabolario e TTR sul nuovo corpus(`new_corpus`), che è 200 token più grande del precedente. Per far sì di avere come risultato una lista di coppie [voabolario, TTR] ho creato due array: in uno inserisco ogni volta i valori del vocabolario e della TTR (dunque ho una sola lista), mentre `complete_voc_TTR_size` è un array che pesca gli ultimi due valori del primo array (`voc_TTR_size`) con `.append` diventando quindi una lista di liste. Per fare ciò ho usato  sul primo array l'operazione di slicing `[-2:]` che prende i valori di un array dal penultimo all'ultimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_TTR(tokens):\n",
    "    i = 200\n",
    "    voc_TTR_size = []\n",
    "    complete_voc_TTR_size = []\n",
    "    while(i<len(tokens)):\n",
    "        new_corpus = tokens[:i]\n",
    "        voc_size = len(list(set(new_corpus)))\n",
    "        TTR = voc_size/len(new_corpus)\n",
    "        voc_TTR_size.append(voc_size)\n",
    "        voc_TTR_size.append(TTR)\n",
    "        complete_voc_TTR_size.append(voc_TTR_size[-2:])\n",
    "        i = i + 200\n",
    "    return complete_voc_TTR_size\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.**Numero di lemmi distinti**. Utilizzo due funzioni:\n",
    "- La prima converte il PoS predefinito di nltk (basato sul Penn Treebank) nel PoS speech corrispondente in WordNet (che ho importato da `nltk.corpus` - vedi la prima cella). In input ho quindi la pos del treebank che, a seconda del suo valore, verrà converita in una PoS di WordNet.\n",
    "- La seconda prende in input i token del corpus e tramite la funzione `nltk.pos_tag` mi restituisce una lista di coppie [parola, PoS]. A questo punto per ogni coppia della lista, lemmatizzo il token a partire dalla sua PoS con  `WordNetLemmatizer().lemmatize`, una funzione di  `nltk.stem ` che lemmatizza le parole basandosi sul database lessicale WordNet. La funzione può anche ricevere in input la PoS di WordNet corrispondente alla parola per essere più precisa nella lemmatizzazione. Per fare ciò uso la prima funzione sulle PoS predefinite di ogni parola,  convertendole in PoS di WordNet. Ogni volta che lemmatizzo una parola, la inserisco in una variabile e poi in un array che le racchiuderà tutte (`text_lem`). Infine utilizzo  ` set` e  ` list` sull'array per eliminare le occorrenze e trasformarlo in una lista, così da poter poi restituire la sua lunghezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(penn_treebank_pos):\n",
    "    if penn_treebank_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif penn_treebank_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif penn_treebank_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif penn_treebank_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def voc_lem(tokens):\n",
    "    text_lem = []\n",
    "    word_pos_list = nltk.pos_tag(tokens)\n",
    "    for token, pos in word_pos_list:\n",
    "        token_lem = WordNetLemmatizer().lemmatize(token, get_wordnet_pos(pos))\n",
    "        text_lem.append(token_lem)\n",
    "    voc_lem = list(set(text_lem))\n",
    "    return len(voc_lem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `main` racchiude tutte le funzioni create in precedenza, applicandole ai due testi in input che gli fornisco (`file_1`, `file_2`) e stampando il risultato. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Confronto del numero di frasi e di token dei due corpora:\n",
      " \n",
      "   Le frasi presenti nel primo corpus sono 2191, mentre i token sono 45144.\n",
      " \n",
      "   Le frasi presenti nel secondo corpus sono 2970, mentre i token sono 88774.\n",
      " \n",
      " - Confronto della lunghezza media delle frasi in token:\n",
      " \n",
      "   La lunghezza media delle frasi nel primo corpus è 20.604290278411685, mentre quella dei token è 3.596314017366649.\n",
      " \n",
      "   La lunghezza media delle frasi nel secondo corpus è 29.89023569023569, mentre quella dei token è 4.149401851893573.\n",
      " \n",
      " - Confronto del numero di hapax nei primi 500, 1000, 3000 token e in tutto il corpus:\n",
      " \n",
      "    Nel primo corpus gli hapax tra primi 500, 1000, 3000 token e in tutto il corpus sono [203, 350, 730, 3788].\n",
      " \n",
      "    Nel secondo corpus gli hapax tra primi 500, 1000, 3000 token e in tutto il corpus sono [158, 237, 508, 3787].\n",
      " \n",
      " - Confronto della dimensione del vocabolario e della ricchezza lessicale per proporizoni incrementali di 200 token:\n",
      " \n",
      "    La dimensione del vocabolario e ricchezza lessicale (TTR), calcolati per porzioni incrementali di 200 token, nel primo corpus è [[120, 0.6], [213, 0.5325], [300, 0.5], [376, 0.47], [451, 0.451], [512, 0.4266666666666667], [569, 0.4064285714285714], [646, 0.40375], [704, 0.39111111111111113], [768, 0.384], [830, 0.37727272727272726], [887, 0.3695833333333333], [937, 0.36038461538461536], [979, 0.34964285714285714], [1023, 0.341], [1068, 0.33375], [1117, 0.3285294117647059], [1171, 0.3252777777777778], [1214, 0.3194736842105263], [1269, 0.31725], [1310, 0.3119047619047619], [1360, 0.3090909090909091], [1415, 0.3076086956521739], [1465, 0.30520833333333336], [1503, 0.3006], [1552, 0.29846153846153844], [1597, 0.29574074074074075], [1642, 0.2932142857142857], [1673, 0.28844827586206895], [1717, 0.2861666666666667], [1766, 0.28483870967741937], [1802, 0.2815625], [1846, 0.2796969696969697], [1894, 0.27852941176470586], [1933, 0.27614285714285713], [1978, 0.2747222222222222], [2015, 0.2722972972972973], [2068, 0.27210526315789474], [2110, 0.2705128205128205], [2148, 0.2685], [2188, 0.26682926829268294], [2226, 0.265], [2265, 0.2633720930232558], [2302, 0.2615909090909091], [2335, 0.2594444444444444], [2384, 0.2591304347826087], [2427, 0.2581914893617021], [2460, 0.25625], [2489, 0.2539795918367347], [2532, 0.2532], [2575, 0.25245098039215685], [2603, 0.2502884615384615], [2630, 0.2481132075471698], [2655, 0.24583333333333332], [2682, 0.24381818181818182], [2724, 0.24321428571428572], [2755, 0.24166666666666667], [2788, 0.2403448275862069], [2801, 0.2373728813559322], [2824, 0.23533333333333334], [2860, 0.23442622950819672], [2884, 0.23258064516129032], [2914, 0.23126984126984126], [2941, 0.229765625], [2968, 0.2283076923076923], [2998, 0.2271212121212121], [3026, 0.22582089552238807], [3049, 0.22419117647058823], [3082, 0.22333333333333333], [3100, 0.22142857142857142], [3124, 0.22], [3148, 0.21861111111111112], [3164, 0.2167123287671233], [3190, 0.21554054054054053], [3218, 0.21453333333333333], [3246, 0.21355263157894736], [3269, 0.21227272727272728], [3299, 0.21147435897435898], [3322, 0.21025316455696202], [3341, 0.2088125], [3362, 0.20753086419753086], [3391, 0.20676829268292682], [3439, 0.20716867469879519], [3471, 0.20660714285714285], [3487, 0.20511764705882352], [3510, 0.20406976744186048], [3537, 0.20327586206896553], [3563, 0.20244318181818183], [3593, 0.20185393258426967], [3623, 0.20127777777777778], [3649, 0.20049450549450548], [3678, 0.1998913043478261], [3709, 0.19940860215053763], [3737, 0.19877659574468085], [3765, 0.19815789473684212], [3786, 0.1971875], [3817, 0.19675257731958762], [3847, 0.19627551020408163], [3875, 0.19570707070707072], [3908, 0.1954], [3932, 0.19465346534653466], [3961, 0.19416666666666665], [3982, 0.19330097087378642], [4014, 0.19298076923076923], [4057, 0.1931904761904762], [4083, 0.1925943396226415], [4106, 0.19186915887850467], [4124, 0.19092592592592592], [4144, 0.19009174311926605], [4176, 0.18981818181818183], [4208, 0.18954954954954956], [4227, 0.18870535714285713], [4257, 0.18836283185840708], [4281, 0.18776315789473685], [4307, 0.1872608695652174], [4338, 0.18698275862068967], [4369, 0.1867094017094017], [4399, 0.18639830508474575], [4415, 0.18550420168067228], [4432, 0.18466666666666667], [4460, 0.18429752066115704], [4481, 0.18364754098360656], [4494, 0.1826829268292683], [4525, 0.18245967741935484], [4544, 0.18176], [4564, 0.1811111111111111], [4582, 0.18039370078740158], [4601, 0.1797265625], [4619, 0.17903100775193798], [4635, 0.17826923076923076], [4655, 0.17767175572519084], [4682, 0.17734848484848484], [4697, 0.17657894736842106], [4721, 0.17615671641791045], [4743, 0.17566666666666667], [4764, 0.1751470588235294], [4791, 0.17485401459854014], [4817, 0.17452898550724638], [4846, 0.17431654676258992], [4862, 0.17364285714285715], [4886, 0.17326241134751774], [4919, 0.1732042253521127], [4949, 0.17304195804195804], [4967, 0.1724652777777778], [4990, 0.17206896551724138], [5015, 0.17174657534246576], [5027, 0.17098639455782313], [5048, 0.17054054054054055], [5067, 0.17003355704697987], [5088, 0.1696], [5111, 0.16923841059602648], [5121, 0.16845394736842106], [5149, 0.16826797385620915], [5165, 0.1676948051948052], [5176, 0.16696774193548386], [5190, 0.16634615384615384], [5221, 0.16627388535031848], [5241, 0.16585443037974684], [5251, 0.16512578616352203], [5267, 0.16459375], [5281, 0.16400621118012423], [5287, 0.16317901234567903], [5298, 0.1625153374233129], [5322, 0.16225609756097562], [5342, 0.16187878787878787], [5361, 0.16147590361445782], [5381, 0.1611077844311377], [5397, 0.160625], [5414, 0.1601775147928994], [5424, 0.1595294117647059], [5446, 0.15923976608187135], [5467, 0.15892441860465117], [5483, 0.15846820809248555], [5499, 0.15801724137931034], [5524, 0.15782857142857143], [5538, 0.15732954545454544], [5554, 0.15689265536723163], [5567, 0.15637640449438203], [5585, 0.15600558659217878], [5605, 0.15569444444444444], [5625, 0.15538674033149172], [5639, 0.1549175824175824], [5653, 0.1544535519125683], [5674, 0.15418478260869564], [5691, 0.1538108108108108], [5713, 0.1535752688172043], [5739, 0.15344919786096256], [5758, 0.15313829787234043], [5771, 0.15267195767195768], [5792, 0.15242105263157896], [5813, 0.15217277486910996], [5836, 0.15197916666666667], [5851, 0.15158031088082902], [5866, 0.15118556701030927], [5887, 0.15094871794871795], [5905, 0.1506377551020408], [5914, 0.15010152284263958], [5934, 0.14984848484848484], [5956, 0.14964824120603015], [5975, 0.149375], [6000, 0.14925373134328357], [6012, 0.1488118811881188], [6030, 0.14852216748768474], [6057, 0.1484558823529412], [6072, 0.14809756097560975], [6086, 0.14771844660194175], [6109, 0.14756038647342995], [6135, 0.14747596153846154], [6156, 0.14727272727272728], [6177, 0.14707142857142858], [6206, 0.14706161137440757], [6218, 0.14665094339622642], [6236, 0.1463849765258216], [6249, 0.14600467289719626], [6272, 0.14586046511627906], [6300, 0.14583333333333334], [6313, 0.14546082949308756], [6324, 0.14504587155963303], [6334, 0.14461187214611873], [6348, 0.14427272727272727], [6366, 0.14402714932126698], [6381, 0.14371621621621622], [6391, 0.14329596412556053], [6410, 0.14308035714285713], [6423, 0.14273333333333332]]\n",
      " \n",
      "    La dimensione del vocabolario e ricchezza lessicale (TTR), calcolati per porzioni incrementali di 200 token, nel secondo corpus è [[123, 0.615], [191, 0.4775], [257, 0.42833333333333334], [317, 0.39625], [371, 0.371], [431, 0.3591666666666667], [476, 0.34], [533, 0.333125], [578, 0.3211111111111111], [635, 0.3175], [667, 0.30318181818181816], [708, 0.295], [748, 0.2876923076923077], [782, 0.2792857142857143], [822, 0.274], [862, 0.269375], [885, 0.26029411764705884], [920, 0.25555555555555554], [956, 0.25157894736842107], [988, 0.247], [1022, 0.24333333333333335], [1062, 0.24136363636363636], [1099, 0.23891304347826087], [1136, 0.23666666666666666], [1163, 0.2326], [1192, 0.22923076923076924], [1226, 0.22703703703703704], [1250, 0.22321428571428573], [1284, 0.22137931034482758], [1309, 0.21816666666666668], [1326, 0.21387096774193548], [1350, 0.2109375], [1377, 0.20863636363636365], [1414, 0.20794117647058824], [1440, 0.2057142857142857], [1476, 0.205], [1517, 0.205], [1549, 0.2038157894736842], [1571, 0.20141025641025642], [1595, 0.199375], [1627, 0.19841463414634147], [1660, 0.1976190476190476], [1687, 0.19616279069767442], [1710, 0.1943181818181818], [1733, 0.19255555555555556], [1755, 0.1907608695652174], [1793, 0.19074468085106383], [1814, 0.18895833333333334], [1838, 0.18755102040816327], [1859, 0.1859], [1876, 0.183921568627451], [1915, 0.1841346153846154], [1933, 0.18235849056603773], [1949, 0.18046296296296296], [1965, 0.17863636363636365], [1983, 0.17705357142857142], [2006, 0.17596491228070174], [2027, 0.17474137931034484], [2053, 0.17398305084745763], [2081, 0.17341666666666666], [2114, 0.17327868852459016], [2139, 0.1725], [2168, 0.17206349206349206], [2186, 0.17078125], [2210, 0.17], [2234, 0.16924242424242425], [2263, 0.16888059701492536], [2291, 0.16845588235294118], [2313, 0.1676086956521739], [2333, 0.16664285714285715], [2353, 0.16570422535211268], [2393, 0.16618055555555555], [2424, 0.16602739726027396], [2448, 0.1654054054054054], [2482, 0.16546666666666668], [2500, 0.16447368421052633], [2522, 0.16376623376623375], [2542, 0.16294871794871796], [2559, 0.1619620253164557], [2585, 0.1615625], [2603, 0.16067901234567902], [2627, 0.1601829268292683], [2640, 0.15903614457831325], [2661, 0.15839285714285714], [2681, 0.15770588235294117], [2703, 0.15715116279069768], [2732, 0.15701149425287356], [2754, 0.15647727272727271], [2768, 0.1555056179775281], [2783, 0.15461111111111112], [2807, 0.15423076923076923], [2826, 0.15358695652173912], [2837, 0.1525268817204301], [2852, 0.15170212765957447], [2870, 0.15105263157894736], [2883, 0.15015625], [2899, 0.14943298969072166], [2915, 0.14872448979591837], [2930, 0.147979797979798], [2939, 0.14695], [2953, 0.14618811881188118], [2969, 0.1455392156862745], [2986, 0.1449514563106796], [2995, 0.1439903846153846], [3013, 0.14347619047619048], [3026, 0.14273584905660378], [3040, 0.14205607476635515], [3067, 0.14199074074074075], [3094, 0.14192660550458716], [3110, 0.14136363636363636], [3124, 0.14072072072072073], [3148, 0.1405357142857143], [3161, 0.13986725663716815], [3181, 0.13951754385964912], [3197, 0.139], [3210, 0.13836206896551725], [3222, 0.1376923076923077], [3249, 0.13766949152542374], [3262, 0.13705882352941176], [3268, 0.13616666666666666], [3286, 0.13578512396694215], [3297, 0.13512295081967213], [3315, 0.13475609756097562], [3330, 0.1342741935483871], [3350, 0.134], [3367, 0.13361111111111112], [3378, 0.13299212598425197], [3390, 0.132421875], [3411, 0.1322093023255814], [3431, 0.13196153846153846], [3454, 0.1318320610687023], [3471, 0.13147727272727272], [3501, 0.13161654135338346], [3524, 0.13149253731343283], [3542, 0.13118518518518518], [3585, 0.1318014705882353], [3610, 0.13175182481751824], [3633, 0.1316304347826087], [3659, 0.13161870503597123], [3682, 0.1315], [3709, 0.13152482269503546], [3738, 0.13161971830985916], [3770, 0.1318181818181818], [3796, 0.13180555555555556], [3815, 0.13155172413793104], [3837, 0.13140410958904108], [3850, 0.13095238095238096], [3869, 0.13070945945945947], [3878, 0.13013422818791948], [3889, 0.12963333333333332], [3905, 0.1293046357615894], [3918, 0.12888157894736843], [3935, 0.12859477124183005], [3946, 0.12811688311688313], [3967, 0.12796774193548388], [3984, 0.1276923076923077], [3991, 0.12710191082802547], [4000, 0.12658227848101267], [4015, 0.12625786163522013], [4027, 0.12584375], [4040, 0.12546583850931678], [4053, 0.1250925925925926], [4069, 0.1248159509202454], [4090, 0.12469512195121951], [4105, 0.12439393939393939], [4124, 0.12421686746987952], [4133, 0.12374251497005988], [4154, 0.12363095238095238], [4169, 0.12334319526627219], [4182, 0.123], [4192, 0.12257309941520468], [4210, 0.12238372093023256], [4221, 0.12199421965317919], [4230, 0.12155172413793103], [4245, 0.12128571428571429], [4258, 0.1209659090909091], [4267, 0.12053672316384181], [4299, 0.12075842696629213], [4314, 0.12050279329608939], [4328, 0.12022222222222222], [4350, 0.12016574585635359], [4373, 0.12013736263736263], [4393, 0.12002732240437158], [4405, 0.11970108695652174], [4435, 0.11986486486486486], [4460, 0.11989247311827957], [4481, 0.11981283422459893], [4493, 0.11949468085106384], [4501, 0.11907407407407407], [4505, 0.11855263157894737], [4524, 0.11842931937172775], [4533, 0.118046875], [4544, 0.11772020725388602], [4552, 0.11731958762886598], [4564, 0.11702564102564103], [4576, 0.11673469387755102], [4589, 0.11647208121827411], [4601, 0.11618686868686869], [4616, 0.11597989949748744], [4630, 0.11575], [4647, 0.11559701492537314], [4669, 0.11556930693069307], [4690, 0.11551724137931034], [4703, 0.11526960784313725], [4719, 0.11509756097560976], [4731, 0.11483009708737864], [4741, 0.11451690821256039], [4748, 0.11413461538461539], [4767, 0.11404306220095693], [4778, 0.11376190476190476], [4793, 0.1135781990521327], [4807, 0.11337264150943396], [4827, 0.11330985915492958], [4837, 0.11301401869158878], [4850, 0.1127906976744186], [4861, 0.11252314814814815], [4871, 0.11223502304147466], [4896, 0.11229357798165138], [4902, 0.11191780821917809], [4918, 0.11177272727272727], [4928, 0.11149321266968326], [4942, 0.11130630630630631], [4951, 0.11100896860986548], [4958, 0.11066964285714286], [4966, 0.11035555555555555], [4978, 0.11013274336283185], [4994, 0.11], [5002, 0.10969298245614036], [5019, 0.10958515283842794], [5026, 0.1092608695652174], [5043, 0.10915584415584416], [5060, 0.10905172413793103], [5077, 0.10894849785407726], [5096, 0.10888888888888888], [5115, 0.10882978723404255], [5144, 0.10898305084745763], [5171, 0.1090928270042194], [5179, 0.10880252100840336], [5185, 0.10847280334728034], [5195, 0.10822916666666667], [5213, 0.10815352697095436], [5216, 0.10776859504132232], [5228, 0.10757201646090535], [5240, 0.10737704918032787], [5250, 0.10714285714285714], [5255, 0.1068089430894309], [5268, 0.10663967611336032], [5279, 0.10643145161290322], [5299, 0.10640562248995984], [5319, 0.10638], [5338, 0.10633466135458167], [5344, 0.10603174603174603], [5351, 0.10575098814229249], [5372, 0.10574803149606299], [5381, 0.10550980392156863], [5394, 0.1053515625], [5410, 0.10525291828793774], [5427, 0.10517441860465117], [5434, 0.1049034749034749], [5438, 0.10457692307692308], [5444, 0.1042911877394636], [5458, 0.10416030534351145], [5471, 0.10401140684410647], [5479, 0.1037689393939394], [5488, 0.10354716981132076], [5500, 0.10338345864661654], [5508, 0.10314606741573033], [5515, 0.10289179104477612], [5523, 0.10265799256505576], [5527, 0.10235185185185185], [5539, 0.10219557195571956], [5554, 0.10209558823529412], [5575, 0.1021062271062271], [5590, 0.10200729927007299], [5600, 0.10181818181818182], [5610, 0.10163043478260869], [5615, 0.10135379061371841], [5621, 0.10109712230215827], [5630, 0.10089605734767025], [5638, 0.10067857142857142], [5652, 0.10056939501779359], [5659, 0.10033687943262411], [5677, 0.10030035335689046], [5691, 0.10019366197183098], [5706, 0.10010526315789474], [5716, 0.09993006993006993], [5730, 0.09982578397212544], [5747, 0.09977430555555555], [5763, 0.09970588235294117], [5778, 0.09962068965517241], [5794, 0.099553264604811], [5804, 0.09938356164383562], [5822, 0.09935153583617748], [5846, 0.099421768707483], [5862, 0.09935593220338983], [5878, 0.09929054054054054], [5891, 0.09917508417508418], [5910, 0.09916107382550336], [5924, 0.09906354515050167], [5935, 0.09891666666666667], [5943, 0.09872093023255814], [5956, 0.0986092715231788], [5972, 0.09854785478547855], [5993, 0.09856907894736842], [6009, 0.09850819672131148], [6020, 0.09836601307189542], [6038, 0.09833876221498371], [6052, 0.09824675324675325], [6073, 0.09826860841423948], [6083, 0.09811290322580646], [6096, 0.0980064308681672], [6114, 0.09798076923076923], [6130, 0.09792332268370607], [6141, 0.09778662420382166], [6151, 0.09763492063492063], [6158, 0.09743670886075949], [6165, 0.0972397476340694], [6173, 0.09705974842767295], [6192, 0.09705329153605016], [6210, 0.09703125], [6228, 0.09700934579439252], [6244, 0.09695652173913044], [6253, 0.09679566563467493], [6260, 0.09660493827160493], [6275, 0.09653846153846155], [6292, 0.09650306748466257], [6309, 0.09646788990825689], [6319, 0.09632621951219512], [6330, 0.09620060790273556], [6344, 0.09612121212121212], [6370, 0.09622356495468277], [6391, 0.09625], [6403, 0.09614114114114114], [6412, 0.09598802395209581], [6425, 0.0958955223880597], [6446, 0.09592261904761905], [6460, 0.09584569732937685], [6475, 0.09578402366863906], [6493, 0.0957669616519174], [6506, 0.0956764705882353], [6524, 0.09565982404692082], [6538, 0.09558479532163743], [6557, 0.09558309037900875], [6577, 0.09559593023255814], [6588, 0.09547826086956522], [6602, 0.09540462427745665], [6620, 0.09538904899135446], [6631, 0.09527298850574713], [6639, 0.09511461318051576], [6650, 0.095], [6669, 0.095], [6677, 0.09484375], [6687, 0.09471671388101983], [6695, 0.09456214689265537], [6701, 0.09438028169014084], [6709, 0.09422752808988764], [6720, 0.09411764705882353], [6734, 0.09405027932960894], [6750, 0.09401114206128133], [6762, 0.09391666666666666], [6775, 0.09383656509695291], [6794, 0.09383977900552486], [6803, 0.09370523415977962], [6812, 0.09357142857142857], [6819, 0.0934109589041096], [6827, 0.09326502732240437], [6841, 0.0932016348773842], [6864, 0.0932608695652174], [6876, 0.09317073170731707], [6888, 0.09308108108108108], [6899, 0.09297843665768193], [6929, 0.09313172043010752], [6944, 0.09308310991957104], [6956, 0.0929946524064171], [6976, 0.09301333333333334], [6998, 0.09305851063829787], [7013, 0.0930106100795756], [7027, 0.09294973544973545], [7051, 0.09302110817941953], [7063, 0.09293421052631579], [7072, 0.09280839895013124], [7086, 0.09274869109947644], [7090, 0.09255874673629243], [7109, 0.09256510416666666], [7117, 0.09242857142857143], [7128, 0.09233160621761657], [7141, 0.0922609819121447], [7152, 0.09216494845360824], [7158, 0.09200514138817481], [7168, 0.0918974358974359], [7176, 0.09176470588235294], [7189, 0.09169642857142857], [7201, 0.09161577608142493], [7211, 0.09151015228426396], [7218, 0.09136708860759493], [7225, 0.09122474747474747], [7239, 0.09117128463476071], [7246, 0.09103015075376884], [7260, 0.09097744360902256], [7281, 0.0910125], [7297, 0.0909850374064838], [7312, 0.0909452736318408], [7335, 0.09100496277915633], [7356, 0.09103960396039604], [7371, 0.091], [7386, 0.09096059113300492], [7405, 0.09097051597051597], [7422, 0.09095588235294118], [7432, 0.09085574572127139], [7449, 0.09084146341463414], [7466, 0.0908272506082725], [7482, 0.09080097087378641], [7493, 0.09071428571428572], [7504, 0.0906280193236715], [7511, 0.09049397590361445], [7518, 0.09036057692307692], [7526, 0.09023980815347722], [7540, 0.09019138755980861], [7547, 0.09005966587112171], [7562, 0.09002380952380952], [7569, 0.08989311163895487], [7579, 0.08979857819905213], [7597, 0.08979905437352245], [7604, 0.08966981132075472], [7620, 0.08964705882352941], [7636, 0.0896244131455399], [7647, 0.08954332552693209], [7659, 0.08947429906542056], [7679, 0.0894988344988345], [7692, 0.08944186046511628], [7700, 0.08932714617169374], [7711, 0.08924768518518518], [7723, 0.08918013856812933], [7752, 0.08930875576036866], [7771, 0.08932183908045976], [7786, 0.08928899082568807], [7803, 0.089279176201373], [7836, 0.08945205479452055], [7859, 0.08951025056947608], [7873, 0.0894659090909091], [7886, 0.08941043083900227], [7892, 0.08927601809954751], [7908, 0.08925507900677201]]\n",
      " \n",
      " - Confronto della dimensione del vocabolario di lemmi:\n",
      " \n",
      "    La dimensione del vocabolario di lemmi nel primo corpus è di 5510 lemmi.\n",
      " \n",
      "    La dimensione del vocabolario di lemmi nel secondo corpus è di 6815 lemmi.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(file_1_path, file_2_path):\n",
    "    \n",
    "\n",
    "    #leggo il contenuto del file e lo inserisco in una variabile\n",
    "    corpus_1 = read_file_content(file_1_path)\n",
    "    corpus_2 = read_file_content(file_2_path)\n",
    "\n",
    "\n",
    "    #conto quante frasi e quanti token contengono i due testi con l'operazione len e inserisco i risultati nelle variabili\n",
    "    text_1_sentences = get_sentences(corpus_1)\n",
    "    text_1_tokens, tokens_in_sentences_1 = get_tokens(text_1_sentences)\n",
    "\n",
    "    text_2_sentences = get_sentences(corpus_2)\n",
    "    text_2_tokens, tokens_in_sentences_2 = get_tokens(text_2_sentences)\n",
    "\n",
    "    number_of_sentences_1 = len(text_1_sentences)\n",
    "    corpus_length_1 = len(text_1_tokens)\n",
    "\n",
    "    number_of_sentences_2 = len(text_2_sentences)\n",
    "    corpus_length_2 = len(text_2_tokens)\n",
    "\n",
    "    print(f\" - Confronto del numero di frasi e di token dei due corpora:\")\n",
    "    print(\" \")\n",
    "    print (f\"   Le frasi presenti nel primo corpus sono {number_of_sentences_1}, mentre i token sono {corpus_length_1}.\")\n",
    "    print(\" \")\n",
    "    print(f\"   Le frasi presenti nel secondo corpus sono {number_of_sentences_2}, mentre i token sono {corpus_length_2}.\")\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "    #calcolo la lunghezza media delle frasi in token\n",
    "    sentence_length_1 = sentence_length(number_of_sentences_1, tokens_in_sentences_1)\n",
    "    sentence_length_2 = sentence_length(number_of_sentences_2, tokens_in_sentences_2)\n",
    "\n",
    "\n",
    "    #calcolo la lunghezza media dei token\n",
    "    token_length_1 = token_length(text_1_tokens, corpus_length_1)\n",
    "    token_length_2 = token_length(text_2_tokens, corpus_length_2)\n",
    "\n",
    "    print(f\" - Confronto della lunghezza media delle frasi in token:\")\n",
    "    print(\" \")\n",
    "    print (f\"   La lunghezza media delle frasi nel primo corpus è {sentence_length_1}, mentre quella dei token è {token_length_1}.\")\n",
    "    print(\" \")\n",
    "    print(f\"   La lunghezza media delle frasi nel secondo corpus è {sentence_length_2}, mentre quella dei token è {token_length_2}.\")\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "    #calcolo del numero di hapax nei primi 500, 1000, 3000 token e in tutto il corpus.\n",
    "    hapax_1 = get_hapax(text_1_tokens)\n",
    "    hapax_2 = get_hapax(text_2_tokens)\n",
    "\n",
    "    print(f\" - Confronto del numero di hapax nei primi 500, 1000, 3000 token e in tutto il corpus:\")\n",
    "    print(\" \")\n",
    "    print(f\"    Nel primo corpus gli hapax tra primi 500, 1000, 3000 token e in tutto il corpus sono {hapax_1}.\")\n",
    "    print(\" \")\n",
    "    print(f\"    Nel secondo corpus gli hapax tra primi 500, 1000, 3000 token e in tutto il corpus sono {hapax_2}.\")\n",
    "    print(\" \")\n",
    "\n",
    "    #calcolo della dimensione del vocabolario e ricchezza lessicale (TTR) per porzioni incrementali di 200 token\n",
    "    voc_TTR_size_1 = voc_TTR(text_1_tokens)\n",
    "    voc_TTR_size_2 = voc_TTR(text_2_tokens)\n",
    "\n",
    "    print(f\" - Confronto della dimensione del vocabolario e della ricchezza lessicale per proporizoni incrementali di 200 token:\")\n",
    "    print(\" \")\n",
    "    print(f\"    La dimensione del vocabolario e ricchezza lessicale (TTR), calcolati per porzioni incrementali di 200 token, nel primo corpus è {voc_TTR_size_1}\")\n",
    "    print(\" \")\n",
    "    print(f\"    La dimensione del vocabolario e ricchezza lessicale (TTR), calcolati per porzioni incrementali di 200 token, nel secondo corpus è {voc_TTR_size_2}\")\n",
    "    print(\" \")\n",
    "\n",
    "    #calcolo del vocabolario di lemmi\n",
    "    voc_lem_1 = voc_lem(text_1_tokens)\n",
    "    voc_lem_2 = voc_lem(text_2_tokens)\n",
    "\n",
    "    print(f\" - Confronto della dimensione del vocabolario di lemmi:\")\n",
    "    print(\" \")\n",
    "    print(f\"    La dimensione del vocabolario di lemmi nel primo corpus è di {voc_lem_1} lemmi.\")\n",
    "    print(\" \")\n",
    "    print(f\"    La dimensione del vocabolario di lemmi nel secondo corpus è di {voc_lem_2} lemmi.\")\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "\n",
    "file_1 = \"Heart_of_darkness.txt\"\n",
    "file_2 = \"Language_Sapir.txt\"\n",
    "\n",
    "main(file_1, file_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a30343d8d0e6c1a63eb34c6277766d715d00494db238f5f72a779190013faf1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
