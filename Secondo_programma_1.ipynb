{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le cinque funzioni permettono di:\n",
    "- leggere il contenuto del corpus e inserirlo in una variabile (`contents`), su cui lavoro per non accedere direttamente al testo\n",
    "- fare il sentence splitting del testo, le cui frasi finiscono nella variabile `sentences`, con la funzione di nltk `nltk.tokenize.sent_tokenize`\n",
    "- a partire dalle frasi (estratte nella seconda funzione), questa funzione seleziona i token per ogni frase con una iterazione e li inserisce con l'operazione `.append` in un nuovo array (`tokens_in_sentences`) dove ogni frase è una lista di liste (ognuna al proprio interno ha i suoi token), mentre con l'operazione `.extend` inserisco tutti i token in una unica lista (`all_tokens`)\n",
    "- restituire, a partire dai token del corpus, il corpus annotato morfo-sintatticamente tramite la funzione di nltk `nltk.tag.pos_tag()`. Ottengo una lista contentente coppie [token, Pos]. La PoS, se non è specificato un tagset, viene calcolata sul Penn Treebank.\n",
    "- restituire una lista contente solo le POS del testo a partire dal corpus annotato. Con un ciclo `for` estrae la PoS da ogni coppia parola-PoS e la inserisce in una nuova lista (`only_POS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "\n",
    "def read_file_content(file):\n",
    "    with open(file, 'r', encoding=\"utf8\") as infile:\n",
    "        contents = infile.read()\n",
    "        return contents\n",
    "\n",
    "def get_sentences(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def get_tokens(text_sentences):\n",
    "    all_tokens = []\n",
    "    tokens_in_sentences = []\n",
    "    for sentence in text_sentences:\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        all_tokens.extend(tokens)\n",
    "        tokens_in_sentences.append(tokens)\n",
    "    return all_tokens, tokens_in_sentences\n",
    "\n",
    "def annotate(tokens):\n",
    "     tokens_POS = nltk.tag.pos_tag(tokens) \n",
    "     return tokens_POS\n",
    "\n",
    "def get_POS_only(tokens_POS):\n",
    "    only_POS = []\n",
    "    for token, POS in tokens_POS:\n",
    "        only_POS.append(POS)\n",
    "    return only_POS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a.**Calcolo della sequenza ordinata in ordine descrescente dei primi 10 PoS, bigrammi di Pos e trigrammi di PoS più frequenti**.\n",
    "La funzione `freq_ngrams_POS` restituisce gli n-grammi di PoS più frequenti. In input ha la lista delle PoS del testo (`POS_list`), il grado degli n-grammi(`grams`) da estrarre e il numero preciso di n-grammi più frequenti, ordinati in ordine decrescente, che voglio (`i`). La prima azione che compie è dividere la lista di PoS in ngrammi secondo il grado che gli ho fornito in input usando la funzione `nltk.ngrams(lista_di_PoS, n = grado)`. A questo punto il valore (la lista di n-grammi) viene inserito in una variabile (`ngrams`), di cui verrà calcolata la distribuzione di frequenza con `nltk.FreqDist()`. Infine della distribuzione di frequenza (ovvero un dizionario [n-gramma: frequenza]) sceglieremo i primi `i` valori in ordine decrescente con `.most_common(i)`.\n",
    "\n",
    "1b.**Calcolo dei 20 sostantivi, avverbi e aggettivi più frequenti**. La funzione prende in input il corpus annotato, un array contentente le POS che mi servono e un indice `i` che mi specifica quanti valori mi deve restituire. Le opzioni sono tre, ognuna per la POS di cui ho bisogno: se la POS che voglio trovare è un aggettivo, inserirò nella variabile `POS` un array con i tag corrispondenti agli aggettivi e accederò alle operazioni per quella specifica POS. Faccio così anche per nomi e avverbi. Le operazioni per ogni POS sono: 1.la funzione scorre il corpus annotato e prenderà i token che hanno una POS che inizia come la POS che mi serve (JJ pe l'aggettivo, NN per il sostantivo e RB per l'avverbio), 2.aggiungerà il token ad una lista `l` contentente i token del testo con quella specifica POS, di cui calcolo la distribuzione di frequenza(`nltk.FreqDist(l)`), e 3. resituirò gli `n` token più frequenti nella distribuzione(`l_freq.most_common(n)`). Inoltre, quando filtro i token di cui ho bisogno nel corpus annotato ho aggiunto che devono essere anche più lunghi di 1, altrimenti mi inserisce anche gli apostrofi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a\n",
    "def freq_ngrams_POS(POS_list, grams, i):\n",
    "    ngrams = nltk.ngrams(POS_list, n = grams)\n",
    "    freq_ngrams_POS = nltk.FreqDist(ngrams)\n",
    "    return freq_ngrams_POS.most_common(i)\n",
    "\n",
    "#1b\n",
    "def freq_POS(tokens_POS, POS, n):\n",
    "    l= []\n",
    "    if ('JJ' in POS and 'JJR' in POS and 'JJS' in POS):\n",
    "        for token, token_POS in tokens_POS:\n",
    "            if(token_POS.startswith('JJ')and len(token)>1):\n",
    "                l.append(token)\n",
    "        l_freq = nltk.FreqDist(l)\n",
    "        return l_freq.most_common(n)\n",
    "    if ('RB' in POS and 'RBR' in POS and 'RBS' in POS):\n",
    "        for token, token_POS in tokens_POS:\n",
    "            if(token_POS.startswith('RB')and len(token)>1):\n",
    "                l.append(token)\n",
    "        l_freq = nltk.FreqDist(l)\n",
    "        return l_freq.most_common(n)\n",
    "    if ('NN'in POS and 'NNS' in POS and 'NNP' in POS and 'NNPS' in POS):\n",
    "        for token, token_POS in tokens_POS:\n",
    "            if(token_POS.startswith('NN') and len(token)>1):\n",
    "                l.append(token)\n",
    "        l_freq = nltk.FreqDist(l)\n",
    "        return l_freq.most_common(n)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Estratti i bigrammi composti da Aggettivo e Sostantivo mostare:**\n",
    "- a. I 20 più frequenti, con relativa frequenza\n",
    "- b. I 20 con probabilità condizionata massima, e relativo valore di probabilità\n",
    "- c. I 20 con forza associativa (Pointwise Mutual Information, PMI) massima, e\n",
    "relativa PMI\n",
    "\n",
    "La prima funzione (`get_adj_noun_bigrams`) estrae i bigrammi aggettivo-ostantivo a partire dal corpus annotato. Attraverso `nltk.bigrams` divido il corpus annotato per bigrammi, così da ottenre coppie di token con le rispettive PoS. A questo punto tra questi token prendo solo quelli che hanno come PoS del primo token un aggettivo (`.startswith('JJ'))` e del secondo un sostantivo (`element_2[1].startswith('NN'))`. Ho aggiunto anche che i token siano più lunghi di così da evitare apostrofi o altri segni. Infine, selezionati i bigrammi di token aggettivo-ostantivo creo un bigramma dove inserisco solo i token del bigramma (`element_1[0], element_2[0]`) e li inserisco in una lista.\n",
    "\n",
    "- a. La funzione `get_freq_adj_noun_bigrams` prende in input i bigrammi aggettivo-sostantivo e un valore nuemrico `i`. Calcola la distribuzione di frequenza dei bigrammi (`freq_adj_noun_bigrams`) e restituisce gli i bigrammi più grandi (`freq_adj_noun_bigrams.most_common(i)`)\n",
    "\n",
    "- b. La funzione `get_cond_prob_adj_noun`prende in input i bigrammi aggettivo-sostantivo, i token del corpus e un valore nuemrico `i`. Calcola la distribuzione di frequenza di tutti i bigrammi del testo (`freq_bigrams`) e dei token (`freq_tokens`). Applicando un ciclo `for`, per ogni bigramma aggettivo-sostantivo calcola la probabilità condizionata (frequenza del bigramma nel corpus/frequenza del primo elemento del bigramma) a partire dalle distribuzioni di frequenza calcolate all'inizio. Poi ogni volta crea un bigramma (`bigram`) dove inserisce la coppia aggettivo-sostantivo come primo elemento e la relativa probabilità condizionata come secondo. Aggiunge ogni bigramma in una lista (`prob_cond_all_bigrams`) che va ordinata in modo decrescente (secondo i valori di probabilità) e di cui vanno presi i primi `i` elementi. La funzione `sorted()` ordina la lista in modo crescente di default, con `reverse=True` la ordina in modo decrescente. Per ordinarla a partire dai secondi valori degli elementi (le probabilità), uso `key` all'interno di sorted che richiama una funzione `take_second`, la quale restituisce il secondo valore di un elemento. Infine, dopo aver creato la lista di bigrammi ordinati in modo decrescente a partire dalle probabilità (`ordered_prob_cond_all_bigrams`), la funzione restituisce gli elementi di questa lista dall'inizio fino ad `i` con lo slicing (`ordered_prob_cond_all_bigrams[:i]`).\n",
    "\n",
    "- c. La funzione `get_PMI_adj_noun`prende in input i bigrammi aggettivo-sostantivo, i token del corpus e un valore nuemrico `i`. Calcola la distribuzione di frequenza di tutti i bigrammi del testo (`freq_bigrams`) e dei token (`freq_tokens`). Applicando un ciclo `for`, per ogni bigramma aggettivo-sostantivo calcola la Pointwise Mutual Information (probabilità del bigramma/probabilità del primo elemento del bigramma per il secondo) a partire dalle distribuzioni di frequenza calcolate all'inizio. Per trovare le probabilità divide infatti le frequenze per la lunghezza del corpus (`len(tokens)`), anche per i bigrammi (numero di bigrammi = numero di token). Poi ogni volta crea un bigramma (`bigram`) dove inserisce la coppia aggettivo-sostantivo come primo elemento e la relativa Pointwise Mutual Information come secondo. Aggiunge ogni bigramma in una lista (`PMI_all_bigrams`) che va ordinata in modo decrescente (secondo i valori di PMI) e di cui vanno presi i primi `i` elementi. La funzione `sorted()` ordina la lista in modo crescente di default, con `reverse=True` la ordina in modo decrescente. Per ordinarla a partire dai secondi valori degli elementi (ovvero la PMI di ognuno), uso `key` all'interno di sorted che richiama una funzione `take_second`, la quale restituisce il secondo valore di un elemento. Infine, dopo aver creato la lista di bigrammi ordinati in modo decrescente a partire dalle PMI (`ordered_PMI_all_bigrams`), la funzione restituisce gli elementi di questa lista dall'inizio fino ad `i` con lo slicing (`ordered_PMI_all_bigrams[:i]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_noun_bigrams(tokens_POS):\n",
    "    tokens_POS_bigrams = list(nltk.bigrams(tokens_POS))\n",
    "    adj_noun_bigrams = []\n",
    "    for element_1, element_2 in tokens_POS_bigrams:\n",
    "        if (element_1[1].startswith('JJ')) and (element_2[1].startswith('NN') and len(element_1[0])>1 and len(element_2[0])>1):\n",
    "            bigram = (element_1[0], element_2[0])\n",
    "            adj_noun_bigrams.append(bigram)\n",
    "    return adj_noun_bigrams\n",
    "\n",
    "#a\n",
    "def get_freq_adj_noun_bigrams(adj_noun_bigrams, i):\n",
    "    freq_adj_noun_bigrams = nltk.FreqDist(adj_noun_bigrams)\n",
    "    return  freq_adj_noun_bigrams.most_common(i)\n",
    "\n",
    "#b\n",
    "def get_cond_prob_adj_noun(adj_noun_bigrams, tokens, i):\n",
    "    freq_bigrams = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    freq_tokens = nltk.FreqDist(tokens)\n",
    "    prob_cond_all_bigrams = []\n",
    "    for token_1, token_2 in adj_noun_bigrams:\n",
    "        prob_cond_bigram = freq_bigrams[(token_1, token_2)]/freq_tokens[token_1]\n",
    "        bigram = ((token_1, token_2), prob_cond_bigram)\n",
    "        prob_cond_all_bigrams.append(bigram)\n",
    "    ordered_prob_cond_all_bigrams = sorted(prob_cond_all_bigrams, key=take_second, reverse=True)\n",
    "    return ordered_prob_cond_all_bigrams[:i]\n",
    "\n",
    "def take_second(elem):\n",
    "    return elem[1]\n",
    "\n",
    "#c\n",
    "def get_PMI_adj_noun(adj_noun_bigrams, tokens, i):\n",
    "    freq_tokens = nltk.FreqDist(tokens)\n",
    "    freq_bigrams = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    PMI_all_bigrams = []\n",
    "    for token_1, token_2 in adj_noun_bigrams:\n",
    "        PMI = math.log2((freq_bigrams[(token_1, token_2)]/(len(tokens)))/((freq_tokens[token_1]/len(tokens))*((freq_tokens[token_2]/len(tokens)))))\n",
    "        bigram = ((token_1, token_2), PMI)\n",
    "        PMI_all_bigrams.append(bigram)\n",
    "    ordered_PMI_all_bigrams = sorted(PMI_all_bigrams, key=take_second, reverse=True)\n",
    "    return ordered_PMI_all_bigrams[:i]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.**Considerate le frasi con una lunghezza compresa tra 10 e 20 token, in cui almeno la metà dei token occorre almeno 2 volte nel corpus, si identifichino:**\n",
    "- a. La frase con la media della distribuzione di frequenza dei token più alta\n",
    "- b. La frase con la media della distribuzione di frequenza dei token più bassa\n",
    "- c. La frase con probabilità più alta secondo un modello di Markov di ordine 2\n",
    "costruito a partire dal corpus di input\n",
    "\n",
    "La funzione `get_specific_sentences` prende in input le frasi divise in token (che ho estratto con la funzione `get_tokens` - vedi sopra) e i token. Per ogni frase, se la frase ha lunghezza compresa tra 10 e 20, conto ogni token con frequenza maggiore di 1 attraverso una variabile `i` che sarà poi azzerata passando alla frase successiva. Se questa variabile sarà maggiore-uguale della metà della lunghezza della frase (quindi i token con frequenza maggiore di 1 sono almeno la metà dei token della frase), aggiungerò la frase in una lista (`specific_sentences`). La funzione restituirà questa lista.\n",
    "\n",
    "- a. La funzione `get_max_sent_freq_dist` prende in input le frasi selezionate con la funzione `get_specific_sentences` e i token. Iterando su ogni frase e poi su ogni token di ogni frase, somma la frequenza di ogni token nella frase (a partire dlla distribuzione di frequenza `freq_tokens`), la divide per la lunghezza della frase e inserisce il valore nella variabile `sent_freq_dist` (che torna 0 pasando alla frase successiva così come la somma delle frequenze dei token `freq_all_tokens`). Se questo valore è maggiore di `max_sent_freq_dist` (che all'inizio sarà 0), la frase diventa `max_sent`, ovvero la frase con la media della distribuzione di frequenza dei token più alta. La funzione restituirà `max_sent`.\n",
    "\n",
    "- b. La funzione `get_min_sent_freq_dist` prende in input le frasi selezionate con la funzione `get_specific_sentences` e i token. Iterando su ogni frase e poi su ogni token di ogni frase, somma la frequenza di ogni token nella frase (a partire dlla distribuzione di frequenza `freq_tokens`), la divide per la lunghezza della frase e inserisce il valore nella variabile `sent_freq_dist` (che torna 0 pasando alla frase successiva così come la somma delle frequenze dei token `freq_all_tokens`). Se questo valore è minore di `min_sent_freq_dist` (che all'inizio sarà infinto), la frase diventa `min_sent`, ovvero la frase con la media della distribuzione di frequenza dei token più bassa. La funzione restituirà `min_sent`.\n",
    "\n",
    "- c. La funzione `markov2` calcola la probabilità di una frase a partire dalle distribuzioni in un corpus. Sia la frase che i token del corpus sono forniti in input. Calcolando la probabilità su un modello di Markov di ordine 2, avrò bisogno di dividere i token e la frase in bigrammi e trigrammi (`bigrams`, `trigrams`, `bigrams_sentence`, `trigrams_sentence`). Calcola quindi la distribuzione di frequenza dei token, dei bigrammi e dei trigrammi del testo (`freq_tokens`, `freq_bigrams`, `freq_trigrams`). Viene poi calcolata la probabilità della prima parola della frase(`token_1_prob`) e quella condizionata della seconda (`token_2_prob`). Poi su ogni trigramma della frase viene calcolata la probabilità condizionata (`trigram_prob`), aggiornando il bigramma nella divisione con una variabile `i`. Infine la funzione restituisce il prodotto di `token_1_prob`, `token_2_prob` e `trigram_prob`, ovvero la probabilità della frase.\n",
    "\n",
    "La funzione `get_max_sent_prob` prende in input le frasi selezionate con la funzione `get_specific_sentences` e i token. Per ogni frase viene calcolata la probabilità con la funzione `markov2`. Se la probabilità è maggiore di `max_sent_prob` (che inizialmente è 0), allora la frase diventa `max_sent`, la frase con probabilità più alta, e la sua probabilità diventa `max_sent_prob`. Dopo aver iterato su ogni frase, la funzione resituirà la `max_sent` tra le frasi date in input, basandosi sulla distribuzione di probabilità del corpus fornito in input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_sentences(divided_sentences, tokens):\n",
    "    freq_tokens =  nltk.FreqDist(tokens)\n",
    "    i = 0\n",
    "    specific_sentences = []\n",
    "    for sentence in divided_sentences:\n",
    "        if len(sentence)>10 and len(sentence)<20:\n",
    "            for token in sentence:\n",
    "                if freq_tokens[token]>1:\n",
    "                    i = i + 1\n",
    "            if i >= (len(sentence)//2):\n",
    "                specific_sentences.append(sentence)\n",
    "            i = 0 \n",
    "    return specific_sentences\n",
    "\n",
    "#a\n",
    "def get_max_sent_freq_dist(specific_sentences, tokens):\n",
    "    freq_tokens =  nltk.FreqDist(tokens)\n",
    "    freq_all_tokens = 0\n",
    "    max_sent_freq_dist = 0\n",
    "    for sentence in specific_sentences:\n",
    "        for token in sentence:\n",
    "            freq_all_tokens = freq_all_tokens + freq_tokens[token]\n",
    "        sent_freq_dist = freq_all_tokens/len(sentence)\n",
    "        if sent_freq_dist>max_sent_freq_dist:\n",
    "            max_sent_freq_dist = sent_freq_dist\n",
    "            max_sent = sentence\n",
    "        freq_all_tokens = 0\n",
    "        sent_freq_dist = 0\n",
    "    return max_sent\n",
    "\n",
    "#b\n",
    "def get_min_sent_freq_dist(specific_sentences, tokens):\n",
    "    freq_tokens =  nltk.FreqDist(tokens)\n",
    "    freq_all_tokens = 0\n",
    "    min_sent_freq_dist = float('inf')\n",
    "    for sentence in specific_sentences:\n",
    "        for token in sentence:\n",
    "            freq_all_tokens = freq_all_tokens + freq_tokens[token]\n",
    "        sent_freq_dist = freq_all_tokens/len(sentence)\n",
    "        if sent_freq_dist<min_sent_freq_dist:\n",
    "            min_sent_freq_dist = sent_freq_dist\n",
    "            min_sent = sentence\n",
    "        freq_all_tokens = 0\n",
    "        sent_freq_dist = 0\n",
    "    return min_sent\n",
    "\n",
    "#c\n",
    "def markov2(sentence, tokens):\n",
    "    freq_tokens =  nltk.FreqDist(tokens)\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "    trigrams = list(nltk.ngrams(tokens, n=3))\n",
    "    freq_bigrams = nltk.FreqDist(bigrams)\n",
    "    freq_trigrams = nltk.FreqDist(trigrams)\n",
    "    bigrams_sentence = list(nltk.bigrams(sentence))\n",
    "    trigrams_sentence = list(nltk.ngrams(sentence, n=3))\n",
    "    prob = 1.0\n",
    "    token_1_prob = freq_tokens[sentence[0]]*1.0/len(tokens)*1.0\n",
    "    token_2_prob = freq_bigrams[bigrams_sentence[0]]*1.0/freq_tokens[sentence[0]]*1.0\n",
    "    i = 0\n",
    "    for trigram in trigrams_sentence:\n",
    "        trigram_prob = freq_trigrams[trigram]*1.0/freq_bigrams[bigrams_sentence[i]]*1.0\n",
    "        prob = prob*trigram_prob\n",
    "        i=i+1\n",
    "    return prob*token_1_prob*token_2_prob\n",
    "\n",
    "def get_max_sent_prob(specific_sentences, tokens):\n",
    "    max_sent_prob = 0\n",
    "    for sentence in specific_sentences:\n",
    "        if markov2(sentence, tokens)>max_sent_prob:\n",
    "            max_sent_prob = max_sent_prob\n",
    "            max_sent = sentence\n",
    "    return max_sent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.**Estratte le Entità Nominate del testo, identificare per ciascuna classe di NE i 15\n",
    "elementi più frequenti, ordinati per frequenza decrescente e con relativa frequenza.**\n",
    "\n",
    "La funzione `get_NE` prende in input il corpus annotato e restituisce un array di coppie (parola, entità della parola). Dopo aver creato l'albero delle entità con la funzione `nltk.ne_chunk()`, per ogni nodo dell'albero estrae l'entità (`entity_type`) e il token a cui corrisponde (`entity`). Per fare ciò pongo che il nodo debba avere un attributo di tipo label (`hasattr(nodo, 'label')`), ovvero che sia un nodo con una NE, e scorro le foglie del nodo (`nodo.leaves()`) per ottenere gli elementi di quella NE. Questi elementi sono però nella forma token-POS, quindi prendo solo il token come `entity` .Infine appende la coppia (`entity`, `entity_type`) all'array NE, che viene restituito come output.\n",
    "\n",
    "La funzione `freq_NE` prende in input le coppie (token, NE) estratte dalla funzione `get_NE`, una classe specifica di NE e un valore numerico `i`. Iterando su ogni elemento della lista di coppie (token, NE), se il secondo elemento (la NE) è uguale alla NE che viene fornita in input, aggiungo il primo elemento (il token) in una lista `l`. Infine viene calcolata la distribuzione di frequenza della lista `l` e ne vengono restituiti gli `i` valori più grandi.\n",
    "La funzione verrà quindi chiamata tre volte, una per ogni `NE_type` diverso (GPE, PERSON e ORGANIZATION).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NE(tagged_corpus):\n",
    "    NE_tree = nltk.ne_chunk(tagged_corpus)\n",
    "    NE = []\n",
    "    for nodo in NE_tree:\n",
    "        if hasattr(nodo, 'label'):\n",
    "            entity_type = nodo.label()\n",
    "            for token, POS in nodo.leaves():\n",
    "                entity = token\n",
    "                NE.append((entity, entity_type))\n",
    "    return NE\n",
    "\n",
    "def freq_NE(NE, NE_type, i):\n",
    "    l = []\n",
    "    for element in NE:\n",
    "         if element[1] == NE_type:\n",
    "            l.append(element[0])\n",
    "    l_freq = nltk.FreqDist(l)\n",
    "    return l_freq.most_common(i)\n",
    "            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `main` racchiude tutte le funzioni create in precedenza, applicandole al testo fornito in input (`Heart_of_darkness.txt`) e stampando il risultato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Le 10 PoS più frequenti sono:\n",
      " \n",
      " [(('NN',), 6534), (('IN',), 5104), (('DT',), 4406), (('PRP',), 3546), (('VBD',), 3218), (('JJ',), 3145), ((',',), 2849), (('.',), 2293), (('RB',), 2275), (('NNS',), 1526)]\n",
      " \n",
      " - I 10 bigrammi di PoS più frequenti sono:\n",
      " \n",
      " [(('DT', 'NN'), 2468), (('IN', 'DT'), 2044), (('NN', 'IN'), 1709), (('PRP', 'VBD'), 1707), (('JJ', 'NN'), 1475), (('NN', ','), 1102), (('DT', 'JJ'), 1102), (('NN', '.'), 950), (('.', 'PRP'), 764), (('IN', 'PRP'), 627)]\n",
      " \n",
      " - I 10 trigrammi di PoS più frequenti sono:\n",
      " \n",
      " [(('IN', 'DT', 'NN'), 1221), (('DT', 'NN', 'IN'), 817), (('DT', 'JJ', 'NN'), 783), (('NN', 'IN', 'DT'), 679), (('.', 'PRP', 'VBD'), 538), (('IN', 'DT', 'JJ'), 492), (('JJ', 'NN', 'IN'), 383), (('DT', 'NN', ','), 378), (('DT', 'NN', '.'), 325), (('NN', '.', 'PRP'), 319)]\n",
      " \n",
      " - I 20 sostantivi più frequenti sono:\n",
      " \n",
      " [('Kurtz', 112), ('man', 105), ('time', 74), ('river', 53), ('Mr.', 49), ('men', 45), ('head', 45), ('eyes', 44), ('manager', 42), ('station', 41), ('something', 40), ('nothing', 38), ('day', 37), ('thing', 35), ('earth', 35), ('way', 35), ('voice', 33), ('life', 31), ('pilgrims', 30), ('feet', 29)]\n",
      " \n",
      " - I 20 aggettivi più frequenti sono:\n",
      " \n",
      " [('little', 59), ('other', 47), ('great', 46), ('last', 45), ('more', 42), ('black', 39), ('white', 36), ('such', 36), ('much', 33), ('first', 32), ('long', 30), ('good', 28), ('old', 27), ('high', 23), ('own', 22), ('same', 19), ('dead', 19), ('big', 18), ('many', 17), ('short', 17)]\n",
      " \n",
      " - I 20 avverbi più frequenti sono:\n",
      " \n",
      " [('not', 229), ('very', 114), ('so', 90), ('there', 74), ('then', 63), ('too', 61), ('just', 41), ('as', 39), ('only', 36), ('again', 36), ('even', 35), ('more', 34), ('still', 33), ('back', 33), ('never', 33), ('well', 31), ('suddenly', 30), ('away', 30), ('enough', 26), ('yet', 24)]\n",
      " \n",
      " - I 20 bigrammi aggettivo-sostantivo più frequenti sono:\n",
      " \n",
      " [(('white', 'man'), 7), (('white', 'men'), 6), (('same', 'time'), 5), (('next', 'day'), 5), (('remarkable', 'man'), 5), (('only', 'thing'), 4), (('big', 'river'), 4), (('black', 'wool'), 4), (('long', 'time'), 4), (('black', 'fellows'), 3), (('young', 'man'), 3), (('sick', 'man'), 3), (('fantastic', 'invasion'), 3), (('low', 'shores'), 2), (('many', 'years'), 2), (('uttermost', 'ends'), 2), (('only', 'man'), 2), (('other', 'day'), 2), (('mysterious', 'life'), 2), (('blank', 'space'), 2)]\n",
      " \n",
      " - I 20 bigrammi aggettivo-sostantivo con probabilità condizionata massima sono:\n",
      " \n",
      " [(('cross-legged', 'right'), 1.0), (('ascetic', 'aspect'), 1.0), (('exquisite', 'brilliance'), 1.0), (('benign', 'immensity'), 1.0), (('unstained', 'light'), 1.0), (('radiant', 'fabric'), 1.0), (('diaphanous', 'folds'), 1.0), (('imperceptible', 'fall'), 1.0), (('venerable', 'stream'), 1.0), (('vivid', 'flush'), 1.0), (('unceasing', 'service'), 1.0), (('gigantic', 'tale'), 1.0), (('sacred', 'fire'), 1.0), (('three-legged', 'thing'), 1.0), (('lurid', 'glare'), 1.0), (('stay-at-home', 'order'), 1.0), (('them—the', 'ship'), 1.0), (('country—the', 'sea'), 1.0), (('glide', 'past'), 1.0), (('disdainful', 'ignorance'), 1.0)]\n",
      " \n",
      " - I 20 bigrammi aggettivo-sostantivo con forza associativa massima sono:\n",
      " \n",
      " [(('radiant', 'fabric'), 15.462246634244352), (('vivid', 'flush'), 15.462246634244352), (('disdainful', 'ignorance'), 15.462246634244352), (('cracked', 'nut'), 15.462246634244352), (('spectral', 'illumination'), 15.462246634244352), (('Falernian', 'wine'), 15.462246634244352), (('flatter', 'noses'), 15.462246634244352), (('inconclusive', 'experiences'), 15.462246634244352), (('Indian', 'Ocean'), 15.462246634244352), (('tentative', 'jab'), 15.462246634244352), (('whited', 'sepulchre'), 15.462246634244352), (('venetian', 'blinds'), 15.462246634244352), (('ungarnished', 'staircase'), 15.462246634244352), (('forty-five', 'seconds'), 15.462246634244352), (('indifferent', 'placidity'), 15.462246634244352), (('_Morituri', 'te'), 15.462246634244352), (('du', 'calme_.'), 15.462246634244352), (('two-penny-half-penny', 'river-steamboat'), 15.462246634244352), (('greyish-whitish', 'specks'), 15.462246634244352), (('untouched', 'expanse'), 15.462246634244352)]\n",
      " \n",
      " - La frase con la media della distribuzione di frequenza dei token più alta è:\n",
      " \n",
      " ['Camp', ',', 'cook', ',', 'sleep', ',', 'strike', 'camp', ',', 'march', '.']\n",
      " \n",
      " - La frase con la media della distribuzione di frequenza dei token più bassa è:\n",
      " \n",
      " ['What', 'did', 'it', 'matter', 'what', 'any', 'one', 'knew', 'or', 'ignored', '?']\n",
      " \n",
      " - La frase con con probabilità più alta secondo un modello di Markov di ordine 2 è:\n",
      " \n",
      " ['“', 'We', 'have', 'lost', 'the', 'first', 'of', 'the', 'ebb', ',', '”', 'said', 'the', 'Director', 'suddenly', '.']\n",
      " \n",
      " - I 15 elementi della classe PERSON più frequenti sono:\n",
      " \n",
      " [('Kurtz', 105), ('Mr.', 49), ('Marlow', 10), ('Don', 9), ('Jove', 6), ('Mind', 3), ('Fresleven', 3), ('Good', 3), ('Towson', 3), ('Absurd', 3), ('Gravesend', 2), ('Sir', 2), ('Light', 2), ('True', 2), ('Deal', 2)]\n",
      " \n",
      " - I 15 elementi della classe GPE più frequenti sono:\n",
      " \n",
      " [('English', 8), ('Europe', 8), ('Russian', 8), ('Kurtz', 5), ('Thames', 3), ('French', 3), ('Swede', 3), ('Nellie', 1), ('Essex', 1), ('Deptford', 1), ('Greenwich', 1), ('Eastern', 1), ('East', 1), ('Destiny', 1), ('Romans', 1)]\n",
      " \n",
      " - I 15 elementi della classe ORGANIZATION più frequenti sono:\n",
      " \n",
      " [('Company', 11), ('Administration', 3), ('Central', 3), ('Station', 3), ('Inner', 2), ('Eldorado', 2), ('Companies', 1), ('Accountant', 1), ('_Golden', 1), ('Queen', 1), ('Highness', 1), ('_Erebus_', 1), ('Ravenna', 1), ('Continental', 1), ('Continent', 1)]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def main(file):\n",
    "\n",
    "\n",
    "    #eggo il contenuto del file e lo inserisco in una variabile\n",
    "    corpus = read_file_content(file)\n",
    "\n",
    "\n",
    "    #faccio il sentence splitting del testo e lo inserisco in una variabile\n",
    "    sentences = get_sentences(corpus)\n",
    "\n",
    "\n",
    "    #calcolo i token e i token per frase del corpus \n",
    "    tokens, tokens_in_sentence = get_tokens(sentences)\n",
    "\n",
    "\n",
    "    #faccio l'annotazione e ed estraggo la lista di PoS \n",
    "    tagged_corpus = annotate(tokens)\n",
    "    POS_list = get_POS_only(tagged_corpus)\n",
    "\n",
    "\n",
    "    #calcolo i primi 10 POS, i primi 10 bigrammi di POS e i primi 10 trigrammi di POS\n",
    "    ten_POS = freq_ngrams_POS(POS_list, 1, 10)\n",
    "    ten_bigr_POS = freq_ngrams_POS(POS_list, 2, 10)\n",
    "    ten_trigr_POS = freq_ngrams_POS(POS_list, 3, 10)\n",
    "\n",
    "    print(f\" - Le 10 PoS più frequenti sono:\")\n",
    "    print(f\" \")\n",
    "    print(f\" {ten_POS}\")\n",
    "    print(f\" \")\n",
    "    print(f\" - I 10 bigrammi di PoS più frequenti sono:\")\n",
    "    print(f\" \")\n",
    "    print(f\" {ten_bigr_POS}\")\n",
    "    print(f\" \")\n",
    "    print(f\" - I 10 trigrammi di PoS più frequenti sono:\")\n",
    "    print(f\" \")\n",
    "    print(f\" {ten_trigr_POS}\")\n",
    "    print(f\" \")\n",
    "\n",
    "\n",
    "    #Calcolo dei 20 sostantivi più frequenti\n",
    "    POS_nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    freq_POS_nouns = freq_POS(tagged_corpus, POS_nouns, 20)\n",
    "    \n",
    "    print(f\" - I 20 sostantivi più frequenti sono:\")\n",
    "    print(f\" \")\n",
    "    print(f\" {freq_POS_nouns}\")\n",
    "    print(f\" \")\n",
    "\n",
    "\n",
    "    #Calcolo dei 20 aggettivi più frequenti\n",
    "    POS_adj = ['JJ', 'JJR', 'JJS']\n",
    "    freq_POS_adj = freq_POS(tagged_corpus, POS_adj, 20)\n",
    "    \n",
    "    print(f\" - I 20 aggettivi più frequenti sono:\")\n",
    "    print(f\" \")\n",
    "    print(f\" {freq_POS_adj}\")\n",
    "    print(f\" \")\n",
    "\n",
    "\n",
    "    #Calcolo dei 20 avverbi più frequenti\n",
    "    POS_adv = ['RB', 'RBR', 'RBS']\n",
    "    freq_POS_adv = freq_POS(tagged_corpus, POS_adv, 20)\n",
    "    \n",
    "    print(f\" - I 20 avverbi più frequenti sono:\")\n",
    "    print(f\" \")\n",
    "    print(f\" {freq_POS_adv}\")\n",
    "    print(f\" \")\n",
    "    \n",
    "\n",
    "    #Estrazione dei bogrammi aggettivo-sostantivo\n",
    "    adj_noun_bigrams = get_adj_noun_bigrams(tagged_corpus)\n",
    "\n",
    "    #Calcolo dei 20 bigrammi più frequenti\n",
    "    adj_noun_bigrams_freq = get_freq_adj_noun_bigrams(adj_noun_bigrams, 20)\n",
    "\n",
    "    print(f' - I 20 bigrammi aggettivo-sostantivo più frequenti sono:')\n",
    "    print(f\" \")\n",
    "    print(f\" {adj_noun_bigrams_freq}\")\n",
    "    print(f\" \")\n",
    "    \n",
    "\n",
    "    #Calcolo dei 20 bigrammi con probabilità condizionata massima\n",
    "    cond_prob_adj_noun = get_cond_prob_adj_noun(adj_noun_bigrams, tokens, 20)\n",
    "\n",
    "    print(f' - I 20 bigrammi aggettivo-sostantivo con probabilità condizionata massima sono:')\n",
    "    print(f\" \")\n",
    "    print(f\" {cond_prob_adj_noun}\")\n",
    "    print(f\" \")\n",
    "\n",
    "\n",
    "    #Calcolo dei 20 bigrammi con PMI massima:\n",
    "    PMI_adj_noun = get_PMI_adj_noun(adj_noun_bigrams, tokens, 20)\n",
    "\n",
    "    print(f' - I 20 bigrammi aggettivo-sostantivo con forza associativa massima sono:')\n",
    "    print(f\" \")\n",
    "    print(f\" {PMI_adj_noun}\")\n",
    "    print(f\" \")\n",
    "\n",
    "\n",
    "    #Selezione delle frasi con una lunghezza compresa tra 10 e 20 token, in cui almeno la metà dei token occorre almeno 2 volte nel corpus\n",
    "    text_specific_sentences = get_specific_sentences(tokens_in_sentence, tokens)\n",
    "\n",
    "    #Calcolo della frase con la media della distribuzione di frequenza dei token più alta\n",
    "    max_freq_sent = get_max_sent_freq_dist(text_specific_sentences, tokens)\n",
    "\n",
    "    print(f' - La frase con la media della distribuzione di frequenza dei token più alta è:')\n",
    "    print(f\" \")\n",
    "    print(f\" {max_freq_sent}\")\n",
    "    print(f\" \")\n",
    "\n",
    "\n",
    "    #Calcolo della frase con la media della distribuzione di frequenza dei token più bassa\n",
    "    min_freq_sent = get_min_sent_freq_dist(text_specific_sentences, tokens)\n",
    "\n",
    "    print(f' - La frase con la media della distribuzione di frequenza dei token più bassa è:')\n",
    "    print(f\" \")\n",
    "    print(f\" {min_freq_sent}\")\n",
    "    print(f\" \")\n",
    "\n",
    "\n",
    "    #Calcolo della frase con probabilità più alta secondo un modello di Markov di ordine 2 costruito a partire dal corpus di input\n",
    "    max_sent_prob = get_max_sent_prob(text_specific_sentences, tokens)\n",
    "\n",
    "    print(f' - La frase con con probabilità più alta secondo un modello di Markov di ordine 2 è:')\n",
    "    print(f\" \")\n",
    "    print(f\" {max_sent_prob}\")\n",
    "    print(f\" \")\n",
    "    \n",
    "\n",
    "    #Estrazione delle NE del corpus\n",
    "    text_NE = get_NE(tagged_corpus)\n",
    "\n",
    "    #Calcolo dei 15 elementi più frequenti della classe PERSON\n",
    "    text_freq_PERSON = freq_NE(text_NE, 'PERSON', 15)\n",
    "\n",
    "    print(f' - I 15 elementi della classe PERSON più frequenti sono:')\n",
    "    print(f\" \")\n",
    "    print(f\" {text_freq_PERSON}\")\n",
    "    print(f\" \")\n",
    "\n",
    "    #Calcolo dei 15 elementi più frequenti della classe GPE (Geo-Political Entity)\n",
    "    text_freq_GPE = freq_NE(text_NE, 'GPE', 15)\n",
    "\n",
    "    print(f' - I 15 elementi della classe GPE più frequenti sono:')\n",
    "    print(f\" \")\n",
    "    print(f\" {text_freq_GPE}\")\n",
    "    print(f\" \")\n",
    "\n",
    "    #Calcolo dei 15 elementi più frequenti della classe ORGANIZATION\n",
    "    text_freq_ORG = freq_NE(text_NE, 'ORGANIZATION', 15)\n",
    "\n",
    "    print(f' - I 15 elementi della classe ORGANIZATION più frequenti sono:')\n",
    "    print(f\" \")\n",
    "    print(f\" {text_freq_ORG}\")\n",
    "    print(f\" \")\n",
    "\n",
    "\n",
    "file = \"Heart_of_darkness.txt\"\n",
    "main(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a30343d8d0e6c1a63eb34c6277766d715d00494db238f5f72a779190013faf1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
